So we've introduced this concept of a recurrent neural network. That recurrent neural network is a very natural extension of the multilayer perceptron. In fact, is simply a a repeated or recurrent usage of the basic a multilayer perceptron. So it's very simple concept. It's also a very powerful concept. However, it's been found that in the context of text, and the context of documents, and the context of natural language, which is what we're trying to analyze here, the recurrent neural network in its most simple form, which we've talked about in the previous lesson, is not as effective as we would like it to be. So now, what we're going to do is we're going to introduce a new form or a more complicated form of recurrent neural network. This is going to also build upon the concept of multilayer perceptron, but in a far more sophisticated way. So I want to say at the start, this lesson is is much more complicated than some of our other lessons. Probably this is the most complicated. The reason I want to spend some time talking about this thing called LSTM, or long short-term memory is because this represents the state of the art in natural language processing today. So while the methodology is somewhat complicated, it's worth studying and worth understanding because this technology is what's driving the recent revolution and natural language processing based upon neural models. This technology is truly remarkable. We'll talk a little bit more about that as we go forward. While this model was more complicated, I hope to demonstrate that it is understandable and perhaps not so difficult to understand. So again, we have the Recurrent Neural Networks. So we see a sequence of words, wn-1, wn, wn+1. This corresponds to the n minus one, the nth, and the nth plus one word in our sequence of a document. We again see the repeated use of the neural network and the repeated softmax. However, this model now has introduced some new things that we did not have in our previous simpler recurrent neural network, and the most important of these are what we call memory cells. So notice that in our simple recurrent net are relatively simple recurrent net. What we did was we propagated the hidden units h from the previous word to the next word. So we had Hn-1 which was feeding into the prediction for hn, which then predicted the nth word. We've introduced something new, which is called a memory cell. We'll describe this in some detail as we go forward. This is represented by c, the cell, Cn-1, Cn and Cn+1, and Cn+2. So we'll we'll drill down into that a little bit more. We again, as before, from the simple simpler recurrent neural net, we still have these hidden units. So we retain those. So the key thing that we've introduced that is new are these memory cells, and we'll drill down more deeply as we go forward. But the key thing to notice is that this setup, this recurrent neural network setup is basically the same. We again see the softmax, which allows us to predict the next word. What's going to change here is what's in yellow, the form of the recurrent net will change. So let's try to understand that. So to do this, we need to kind of take a little bit of a step backwards, and just kind of recall some notation because we'll need to use this notation as we go forward. So this is a bit of a review. So we previously had a word vector which represents a word vector for the n-1 word, and we had a hidden vector, which was representative of the n-1 word. We concatenated those, which means we just stuck them together. Those two vectors are stuck together. After that concatenation process, we call that cumulative vector Xn-1. That vector then goes into a neural network which is characterized by parameters W, and the we, so basically we take the parameter the components of the vector Xn-1, we multiply them by the weights, we sum them together, we get the outputs. We get the outputs we add a bias, and then that process is then sent through a non-linear function f which we'll talk about a little bit more. Then from that, we get are hidden units Hn. So this is the basic construct that we looked at previously. So now what I'd like to do is to just spend a moment talking about the form of that non-linear function. So previously when we introduced the recurrent neural network, we use the hyperbolic tan or tanh. The hyperbolic tangent. It's a widely used function in neural networks. It has a functional relationship which is shown in the figure. So the key thing to take away from that figure is that the hyperbolic tangent will always yield a number between minus one and one. The output of the hyperbolic tangent here represented by y is always between one and minus one. When the input is large positive, when when x is large and positive, the hyperbolic tangent tends towards one. Whenever x or the input is negative and large, the output tends towards negative one. This is a hyperbolic tangent. This is one example of that non-linear function f. We're going to use multiple forms of that function f, and so, therefore, it's worth spending a few moments talking about them. So one is the hyperbolic tangent. Another is another function that we have talked about elsewhere in our lectures, and that is the sigmoid function. So here the sigmoid function is represented by the sigma. That's called that Greek symbol is called sigma, and, therefore, we choose it to represent the sigmoid function. The sigmoid function is shown in the figure. So now notice importantly that this sigmoid function always lives between zero and one. The output of this sigmoid function is always between zero and one. When the input is large and positive, the output of the sigmoid function tends towards one. When the input towards the sigmoid function is negative and large, the sigmoid function has an output which tends towards zero. So it's important to notice that the hyperbolic tangent has an output which is between minus one and one, where the sigmoid function has an output which is between zero and one. Those differences are going to be important as we move forward in our in our discussions. Okay, so let's go back to this model. So this is now our recurrent neural network, and I want to spend a little bit of time walking through the neural network slowly and carefully because this is somewhat complicated. But it is understandable. But we need to go slowly and think about it. I also want to reiterate that this is the state of the art model for natural language processing. So it is certainly worth trying to understand. So again we're going to introduce the notation where Xn-1 is going to be the concatenation or the a combination of the vector for word n-1 and the hidden unit, Hn-1 from the previous step. So this is the same as we did before. So Xn-1 which is going to be the input to our model is the word vector from the previous word and the hidden vector from the previous word. Now what we're going to do is introduce not one neural network but actually three neural networks. Okay so this is where things start to get complicated. Previously, in our recurrent neural network, we had one neural network process. Here we introduce three. One of them is represented by i, the other represented by f, and the other represented by o. We"ll explain what i, f, and o mean later. But if you look at what's going on here, this is not as wildly different as one might think from what we have done before. We take the input Xn-1, and we send it separately through three different neural networks. The parameters of the neural networks associated with the output i, or W sub i and b sub i where b sub i is the bias and Wi The weight parameters, we take the input X n minus one which is our concatenated vector. We multiply it by our weights. We sum them together, and then, we get an output which is then sent through a sigmoid function. We take the input X n minus one we multiply it by the weights associated with W sub i. We add those together. We add the bias. We send those through the sigmoid function and we get an output i sub n. The thing I want to emphasize or I want you to think about is that sigmoid function is operating on a vector. So, the outputs of W i times X n minus one that is a vector, it's like the hidden vector in our neural network. So, the sigmoid function is operating separately on each of the components that are the output of the neural network on the bottom left. So then, for the neural network associated with f, we have parameters W f and we have biases B f. We'd play the same game with the neural network, but now again, our nonlinear function is the sigmoid function. Sigma and then finally, the same thing for the output of the o or which will be corresponding to an output. So, the key thing to notice is that we're now introducing three neural networks. The basic functional form of each of those neural networks is the same. The detailed parameters are different for each of those three neural networks. So again, I want to emphasize that the non-linear function is the sigmoid function, which means that every component of the vector i, the vector f and the vector o. Those are each vectors. So, i sub n is a vector, f sub n is a vector, and o sub n is a vector. These are the hidden vectors that we saw previously in the multilayer perceptron. The thing I want you to notice is because each component of the vector is going through a sigmoid function, and because of the fact that the sigmoid function always has an output between zero and one, we know that each component of the vector i, the component the vector f and the vector o. Each component of those vectors is always between zero and one, that will be important later. So, these three neural networks which have outputs i, f, o, are what we call control networks, control neural networks. They're characterized by the set of parameters Wo, Wf, Wi. So, those are the set of weights and then the set of biases, Bo, Bf, and Bi. So basically, each of those three neural networks looks very much like what we've seen before in the multilayer perceptron. The three control neural networks that we represent here are going to be used to control the output of our neural network or control the output of our recurrent neural network as we'll see as we go forward. Okay, so now, we're going to introduce a fourth neural networks. So, with the LSTM or the Long Short-Term Memory. We're going to have four neural networks. So, three of the neural networks are shown at those three equations. These are the basic multilayer perceptron that we've seen before. Now, we're going to introduce a fourth which is going to give us a new estimate of our memory cell, which I'm going to represent as C sub n. So, this C is our memory cell and the subscript n means it's going to be for the nth one and the squiggly line on top of the sea that's called a tilde system notation. So, that squiggle on top of the sea is called a tilde. So, that's called tilde C n. What we're going to be using here is the hyperbolic tan as the output or the non-linear function in our network. So, in this case, we know that the every component of the vector C tilde n, it's going to be between minus one and one, as manifested by the properties of the hyperbolic tan. So now, what we're going to do the last step, which is a key step which will go through slowly and we'll try to really understand is, we're going to update the memory cell. So, remember that in this recurrent network, we have a hidden vector H n and we have a memory cell C n. Okay. So, let's now try to understand what this symbol. So, C this circle with a dot in it. So, if you look at what we're doing here, we're saying that C n, which is the nth realization of the memory cell, is a combination of C n minus one, which was the memory cell from the previous time. Then C tilde n which is our updated representation of the memory cell. So, C n is a combination of the previous memory cell C n minus one, and a new estimate which is C tilde n. Then, C n minus one and C tilde n, are operated on by the output's f, n and i from our control network. So, right now notation is very confusing. So, let's try to understand what this means. So, let's think about this, f n is the output of one of the control networks that we looked at previously. Remember f n is a vector where every component of that vector is a number between zero and one because of the fact that the f n was manifested by going through the sigmoid function. So, f n is a vector. It's represented by each of the components in those rectangles those are the numbers. Then C n minus one is the memory cell from the previous time stamp n minus one. The symbol circle with a dot, what this means is that we're going to take every component of f n, and multiply it by the corresponding component of C n minus one. Then after we do this, we're going to manifest a new vector which is the combination of f n and C n minus one. So, we take the vector f n which is a set of numbers each of which is between zero and one. We multiply it by the respective component of the memory cell, C n minus one and that gives us a new vector. That process, that operation is represented by f n circle.Cn minus one. So, this is simply taking every component of f n and multiplying it by the corresponding component of C n minus one. So now, you can start to see where this control process is manifested. So, if we go back and look at this, what the f n is doing, it's controlling the contribution of the previous memory cell C n minus one in our update of the new memory cell C n and then the i n, which we'll talk about more in a moment, i n does a similar operation on C tilde n. Then we sum those two vectors together and then that gives us C n. So basically, what's happening is that, memory cell C n is a combination of the previous memory cell C n minus one and the new estimate of the memory cell C tilde the n. Then, the vectors f n and i n each of which is themselves and output of a neural network are controlling the degree to which C n minus one and C tilde n contribute to the new updated cell. The final thing that we need to think about is how we manifest the hidden unit or the output h n which then goes into the softmax. This is now the third control network which has an output o sub n. This is multiplied in the same way as before dot circle with the hyperbolic tangent of the memory cell. So, if you think about what is going on here, we have three control networks that are manifesting the output vectors o n, f n, and i n. We have an updated estimate of our memory cell C tilde n which also is represented by a neural net. Then, the three control networks are then operate on the memory cells. Then, the output o sub n control vector operates on the hyperbolic C n to give us our hidden unit h n. So, this is the basic construct of the Long Short-Term Memory. This is as I've said, the state of the art method for natural language processing and for many types of sequential modeling. It is certainly more complicated than anything that we've looked at before. So, what we're going to do in the next lesson is to walk through this model slowly, part by part to try to understand more fully how it works