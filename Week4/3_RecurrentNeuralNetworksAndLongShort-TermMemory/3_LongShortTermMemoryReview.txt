We've introduced this concept of the long short-term memory. This is a recurrent neural network which is used for modeling of sequences of data. Language which is represented by a sequence of words is a natural place for the long short-term memory to be applied. Within the context of natural language processing each word in the sequence of words of a document is represented by a vector, and so therefore in the context of documents the long short-term memory is operating on a sequence of vectors associated with the words in the document. Now the LSTM is the state of the art method for natural language processing today, and it has been used to achieve remarkable results in many settings. However, it is certainly more complicated than most anything else we've discussed in these lessons, and so therefore it is probably worth taking a step back to review the LSTM understand, and perhaps a little bit more deeply what the various components of the model are doing. So let's recall the long short-term memory, the LSTM. It's composed of three control neural networks, which have output vectors at time n in our sequence. On fn and In. Each of those three neuro networks, have the same basic form in terms of essentially a multi layer perceptron, and we noticed that the non-linear function for each of these control units is a sigmoid function sigma which means that, each of the components of the vectors on, fn and in are numbers between zero and one. In the long short-term memory, we've introduced a new vector in addition to the hidden vector h, we've introduced a memory cell which is represented by c. The memory cell is updated by yet a fourth neuronetwork which is which has a non-linear function corresponding to the hyperbolic tangent. Again a multilayer perceptron, but then the non-linear function is a hyperbolic tangent, which means that c till to n has components or has elements in the vector c till to n which are each a number between minus one and one. Those four neural networks in the top block are then used to update the memory cell cn, and to also update the hidden variable hn. The way that this is done, is by taking the control outputs from the f network and the i network to operate on the previous memory cell cn minus one and our new estimate c till to n. Then the output of that which is cn, our new updated memory cell is then sent through a hyperbolic tangent, and then is operated on by the output of the network with on, and that gives us the hidden unit. So a lot going on here, let's try to understand this a little bit more. So looking at the output of our update of our memory cell, we notice that what the vector fn is doing is it's controlling the degree to which we forget old memory cell components. So remember, that every component of the vector fn is between zero and one. So if the particular component of fn is near zero, then the corresponding component of the memory cell cn minus one is multiplied by a number which is near zero which means it essentially forgets that memory cell output. So, the fn allows the model to forget or to erase previous memory cell outputs. So, the reason we use the symbol f sub n is because this is a methodology for allowing the model to forget or to erase old memory cells. By contrast the in, controls the input of our new estimate c till to n, and so what we're doing is we're taking the previous memory cells, which is a vector of numbers cn minus one, we're multiplying them component by component by the vector fn which is telling us, and remember, that each one of those outputs of fn is between zero and one if a component of fn is near zero that means we're seeking to erase or forget the corresponding component of the memory cell. So F is called our for getting control, i is called our input control because it's controlling the degree to which our new estimate c till to n contributes to our new estimate of cn, and so again, every component of in is between zero and one. If the corresponding component of in is near zero, then the corresponding component of c till to n is not going to contribute significantly to our new representation of the memory cell. By contrast if a component of in is near one, then that means that the corresponding component of our new estimate c till to n is going to contribute significantly to our new memory cell cn. So the forget control in the eye, or the input control are controlling the degree to which our previous memory cell, and our new representation of the memory cell contribute to update the memory cell cn on the next time timestamp. Then the on is the output control, and what it's doing is it's taking the memory cell cn, it's sending it through a non-linear function hyperbolic tan which yields an output between minus one and one, and then the on is also a vector of numbers each of which is between zero and one, and each of the components of on are represented by the respective components of the output of the hyperbolic tan, and then that product of the output by the hyperbolic tan of Cn, controls the degree to which each of those components contribute to the next hidden vector hn. We then introduce a fourth neuronetwork which is our update of our memory cell which is represented by c till to n. So the simple single neural network in the original recurrent neural network. In the LSTM is controlled now by four neural networks, which allow the model to forget previous data in our sequence of data through the f control. They allow the ability to input new data through the eye control, and then the o control, controls the degree to which the memory cell goes to our output, here the hidden vectors. So we've introduced the same basic concept of recurrent model because this basic construct that we see in this schematic repeats recurrently, it's why this is called a recurrent neural network. However, it's got two outputs, it's got the hidden vector hn and it's also got the update of the memory cell cn. All LSTM parameters can be learned using unlabeled data, because basically, what this is is a model that allows us to predict the next word based upon the previous word, and we can do this for a sequence of words in a document or corpus, and there's no need for human labeling of the documents. So therefore, we can apply this model to unlabelled data. The word vectors may be treated as additional parameters in the model, and they maybe learned as well. So this LSTM model is a model that allows us to learn the word embedding vectors and then also allows us to learn a model which is capable of synthesizing text, which we'll talk about more in a moment. This ability to synthesize text is critical for applications like text translation. So in that case, the input text might be a sequence of English words, and then the output might be a sequence of French words. If we're going to translate from English to French, we have to be able to synthesize language, and this LSTM which allows us to synthesize a sequence of words is a way by which we can do this task of texts synthesis. This model, the LSTM is undoubtedly complicated, but it's built up of basic constructs that we have seen before the multilayer perceptron. It's a complicated model, but the reality is that it works very well in practice providing state of the art results for many applications including translation. In the moment, in our next lesson we'll show another application where the LSTM has proven to be very effective for text synthesis.