So, we've introduced some basic models for natural language processing with neuro networks, the Skip-Gram model and the bag-of-words model and these are very nice models for learning those word vectors or those so-called word embeddings. But, there are other things that we might want to do which we cannot do with those tools and so this motivates something called the recurrent neural network. The recurrent neural network we're going to use here for text, it's actually a basic framework for representation of a sequence of data, so the recurrent neural net, recurrent means repeated over and over again and that will become clear as we move forward. But this recurrent neural network concept is a general one, it is not only used for text for natural language modeling but natural language modeling is a very important application. So, let's assume that we have learned our word embeddings or our vectors and now our goal might be to use them to perform text synthesis. So, what we're talking about here is the ability to automatically produce a text. So,examples where this might be useful, are if I give you an image and your goal might be to automatically manifest a caption for that image, or to do language translation, to go from one language to another. So, if we think about that if we go from language A to language B, then we need to be able to synthesize the text and language in language B and so this requires the capacity to for synthesis, so thus far we really don't have those tools and so now we're going to dive in into understand them a little bit to try to understand how we might do text synthesis and this is done using something called RNN or Recurrent Neural Network. So recurrent neural networks as I said, are very powerful tools for text but they're also very powerful tools for anytime we're dealing with sequential data, so therefore they're an important tool to try to understand. So, this is the basic way that we might think about the use of a recurrent net. So, remember that W_n represents the nth word in a document and we may think of a document as a sequence of words, so here words were W_n minus one, word W_n, word n plus one et cetera. So, a sequence of words in a document. Each of those words is going to be represented by a vector of the concept that we have talked about previously, so each word is mapped to a vector and so those words, so what our goal is to have a model which can take the n minus one word in the vector associated with the n minus one word in, send it into a neural network, also use knowledge of the hidden vector at time at the previous word, so h_n minus one and then to be able to predict the nth word. So, the idea here which we will dive deeper in, so this will become more clear as we go forward, is that what we want do is we want to build a model such that given word in a sequence we can predict the next word, so for example given word n minus one we can predict word n, given word W_n, we can predict word W_n plus one. So, what we want to be able to do is to predict a sequence of words one after the other and so this is done with a neural network which is repeated over and over again, so right now this point is this figure might look a bit confusing, we'll dive deeper and it'll become more clear as we go forward. The most important thing to notice here is the recurrent nature of this setup, so notice we have a recurrent or repeated use of the neural network and repeated use of the Softmax and so that's why this is called a recurrent neural network. So, let's try to understand this neuro network a little bit more deeply. So, the neural network is characterized here by two inputs, so W_n minus one is the n minus one word in a document and we're going to represent that word via a vector or by an embedding. We're also going to assume that we have access to the hidden vector in the multilayer perceptron from the word or the previous word or the n minus one word again, those both are going to be concatenated together, so what we're going do, is we're going to take a vector for the n minus one word, so the vector associated with n minus one word one word and then we're going to take the vector associated with the hidden units associated with the previous word, we're going to concatenate them which means we're just going to stick them together and then we're going to send them into a neural network. The neural network is characterized by parameters W and then from this model we will predict the nth hidden units which again are represented by a vector, so the key thing to notice here, is that we have a word vector and the hidden units from the previous word are going to go into a neural network which is characterized by parameters W and then we're going to predict the hidden unit at the network associated with the next word or the nth word, so we're using information from the n minus one word to predict what's going to happen for the nth word, at this point we characterize that in terms of a hidden vector h_n. So, that hidden vector h_n is then going to be sent in the same way that we've done before into the multilayer perceptron and then to the Softmax and then it's going to predict what the nth word is, and then h_n is then sent forward to the next word in the sequence such that we can repetitively or in a recurrent way predict each word in the document. So, notice that the h_n minus one which is the hidden vector from the previous word, feeds into the model, the neural network model, for the prediction of the nth word and then in the process we get a new hidden vector h_n which is then sent forward into the next neuro network and then the associated next word W_n is also sent into that neural network and then they predict the next hidden unit h_n plus one, which then predicts the next word and this repeats or re-occurs and so therefore this is called a recurrent neural network. The key thing to notice though, that this architecture looks very much like what we've seen before and so one of the things that I hope is becoming more evident as we look at these neuro networks over and over again is that the basic structure of this recurrent net is a multilayer perceptron with some inputs predicting an output, our previous discussions of the Skip-Gram and the bag-of-words model had the same basic multilayer perceptron architecture. So while these are different models and they do different things, the heart of them, the underlying heart in each case is the multi-layer perceptron. So, just to introduce a little bit of notation which will be convenient later. So, we take the vector associated with the n minus one word and we take the vector associated with hidden the hidden vector associated with n minus one word, h_n minus one. We concatenate those together and what we're going to do is call the concatenation of those X sub n minus one. So that's just notation that X sub n minus one is simply notation for the concatenation of the vector for word n minus one in the hidden vector n minus one, this is then sent to a model for the hidden vector h_n which is a set of weights W multiplied by the input X_n minus one, we weight each of the inputs and then add it altogether, we add a bias and then we send that through a non-linear function, which here is called the Hyperbolic tangent, this is not the only non-linear function we can use, but it's a very popular one, we'll discuss this in greater detail later, so that is the model of our hidden units h_n, and then the hidden units h_n are then sent through a set of weights, so they're multiplied by weights and then added all together this is the process U h_n, we then add a bias and then this last step is the Softmax and through this model, through this two step where we take the input, we map it to a hidden vector and then the hidden vector is mapped via the Softmax to probabilities, this is our model and so the equations here are just simply ways of representing that and so what the model is doing the last equation is predicting what word n is W_n given the previous word W_n minus one and the previous hidden vector and this is what the multilayer perceptron is doing. So, the key thing to think about as far as what this model is doing, it's predicting the next word the nth word based upon two inputs, it's using the hidden vector h_n minus one from the previous word, and if you think about that what that is basically doing is it's telling us contextual information about which words were likely previously in the previous step, because if you look at this model the hidden vector h_n predicts which word is next and so if we think about what h_n minus one is doing, what it's doing is it's telling us which words were probable in the previous step, tells us contextual information the W_n minus one tells us what was exactly the previous word and so based upon the context provided by h_n minus one and based upon the previous word W_n minus one, we then predict what the next word W_n is. So, this is called a recurrent neural network the reason it's called a recurrent neural network is that this process represented by this figure is repeated over and over again in a recurrent fashion and this is a way in which we can synthesize or generate a sequence of data, here a sequence of words or text.