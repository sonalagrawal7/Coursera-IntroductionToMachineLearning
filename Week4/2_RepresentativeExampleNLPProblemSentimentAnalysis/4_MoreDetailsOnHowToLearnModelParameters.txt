So, now having introduced two of the simple forms of models for learning, the skip gram and the bag of words model, I'd like to dive a little bit deeper into the concept of learning. So, of course this whole subject is about machine learning, and so we'd like to understand in more detail what that means. So, here our machine if we think about this as machine learning. The machine is the algorithm or the model that we've developed for the text, which is here. The principle concept is the idea of mapping each word to a vector. So, now, we want to dive a little bit deeper into what we mean by learning. So, by learning what we mean is we are going to take a corpus of data, so here a large set of documents and then based upon those documents we're going to try to learn the parameters of our model. So, this is called unsupervised learning, and the reason that it's unsupervised, what we mean is that, we're just going to take a corpus of documents and learn this predictive model. We don't need any human to label the meaning of the documents. So, the parameters of our model, the model I want to emphasize and you'll see this repeatedly in different parts of the lectures, or the lessons is this multi-layer perceptron. Is a very fundamental concept also called a neural network which is appearing in many places in what we're doing and will continue to appear in other aspects of the lessons. So, the parameters that we seek to learn are the parameters of this model. So, the model parameters are the set of vectors that represent each of the words which are represented by c. Then, we have a set of parameters of the multi-layer perceptron represented by W and U. Then, based upon this softmax model, we can then predict based upon the input word c represented by c here, we can predict which of the V words are most likely to be present next, and that is represented via this softmax model. So, in this model, we have an input and we have an output, and what we would like to do is to build a model such that whenever we give the input here, the input is represented by the vectors of the surrounding words. So, here we have six surrounding words, surrounding the nth word. We're going to average the vectors associated with them that will be our input, and then what we would like to do is to predict the output which is the presence of the nth word. So, we could do this with the CBOW, or the bag of words model, we can do this with the so-called Skip-Gram model. In both cases, what we're trying to learn is basically the same thing. We're trying to learn the parameters of the multi-layer perceptron and we're also trying to learn the word vectors. These two different models, the skip gram, and the bag of word models are just two different ways of doing that we will see other ways. So, this concept of learning word vectors can be done many different ways. But, no matter which way we use, our goal is to do model learning. So, now let's think about this a little more. So, basically, what we're trying to do is to predict the probability of words on the output. So, if we think back to this softmax and we think about this model, we think about to the right, what we're doing is that we have the probability of each of the V words based upon an input word c represented by the vector c. This is a fundamental concept whether we're using the bag of words or the skip gram model. The parameters are represented by Theta, and so here what we have is, we have the probability of the output words represented by our neural network, manifested via the softmax, and then we have an input word which represents the contextual word or words that are defining the input to the neural network. Then we're going to represent the input word or words by a vector, and then finally we have parameters theta, which are again the word vectors and then the parameters of the multi-layer perceptrons. So, what we're assuming here when we do learning is that, we assume that we're given m examples of pairs. So, if we look at the third line on the third bullet, we were assuming that we have m examples in our corpus where for each example the i'th example we have an input WN, which is the the input vector, and the i means ith example, and then Wout is the output vector again the ith example. We have m of these examples. This is what we call our training set. M here which is the number of examples and if we think about the Skip-Gram and the CBOW concept. This set of m and examples of in and output can be manifested just by looking at a large corpus of documents, and in that setting the number m the number of examples can be quite large. Then, what we do is we set up something which is called a cost function, and so, then that's represented by f, where f is a function of the parameters data and the data. The way that we represent this is simply the sum of the logs of those probabilities for all of the m examples. So, that the last equation is simply the sum of the predictions of our models for all input-output pairs in our training set and what we would like to do is to learn the parameters or seek the model parameters theta such that we maximize that function f. So, if we can do this and this is at the heart of the concept of learning, if we can do this well then that means that we have a model which is good at predicting the output words given the input words in here based upon m examples from our corpus.