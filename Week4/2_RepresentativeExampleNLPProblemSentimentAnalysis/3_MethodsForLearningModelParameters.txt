So, we have introduced the concept of mapping each word in our vocabulary to a vector where a vector is just a set of numbers. So, every word in the vocabulary is mapped to a vector, and so consequently every document which is a series of words is represented by a series of vectors. Once we have that representation of a document in terms of vectors and numbers, we're then prepared to do analysis. But before we look into that analysis, the open question is, how do we learn the word embedding? So, those word vectors themselves. So, we're going to spend a little time focusing on that. It turns out that there are many different ways to learn these word vectors and so we're going to go through several of these. So, this is a very active area of research and continues to this day, but there are several methods that are now widely used. So, let's try to see how this might be done. Imagine a situation for which you have a document, the document is composed of a set of words. So, let's imagine a word n in a document. What we'd like to do is to predict what word n will be based upon the words in the neighborhood of it. For example, we might wish to predict what word n is based upon what word n minus two, n minus one and plus one and n plus 2 are. So, these are the two words before and the tours after word n. What we'd like to do is to build a model that is capable of predicting what the nth word is here based upon to two words before and the two words after. So, what we're going to do is worth understanding that our goal is to map each word to a vector. What we're going to do is we're going to assume that we have the vectors corresponding to the four words in the neighborhood of the nth word. So, assume that we have the word vectors for the two words before and the two words after. What we're going to do is simply average those word vectors and then we're going to send that vector of m numbers, so we're assuming an m dimensional vector. So, this is an m dimensional vector that is manifested after averaging the four words in the neighborhood of the nth word, and then we're going to send it through our familiar multilayer perceptron which is characterized by parameters W and U, and then at the end of that multilayered perceptron, we've talked about the concept of a softmax. The softmax is a way for which we can quantify the probability of each of the V words in our vocabulary. So, what we have now is a model which is going to take two words before, two words after the nth word. We're going to average the word vectors assuming we know what they are. Those are going to be represented in terms of a code which is composed of the components C_1 through C_m. We send that through a multilayered perceptron and then at the end of that multi-layer perceptron, it goes through a softmax which tells us the probability that word n is each of the v possible words in our vocabulary. So, this now allows us to build a predictive model for the probability of which word is our nth word. To give some more clarity on this, this is a piece of text that was written by Ernest Hemingway, and what we're looking at is a situation now where I have a neighborhood of words in green: three words before and three words after. What we would like to do is to predict the presence of the word game based upon the words: there, was, much, hanging outside, the. So, using our model, we have the words: there, was, much, before and hanging, outside, the, after we assume that each of those words is mapped to a vector. We average those vectors. We then from that average get a single vector which has M components that we represent as C_1 through C_m. Those components or those numbers are then sent through a multilayer perceptron. At the top of that multi-layer perceptron is a softmax that tells us the probability of each of the V words in our vocabulary being the missing word here game. Then, what we would like to do is to build a model such that the parameters of the model, which are characterized by the parameters of the multilayered perceptron as well as the word vectors yield good predictions. In other words, we want our model to be good. In this case, at predicting game as the missing word. If we can do this well, effectively, we will have learned very good vector representations for each word. So, for this to work, we have to learn good representations of each word in terms of a vector and that indeed is our goal. Another approach which is kind of a complement is, let's assume that the nth word is given to us, and let us assume that the nth word is represented by the vector with components C_1 to C_m. We again send this vector through our multilayer perceptron. At the top of the multilayered perceptron again, is the softmax, and then now what we would like to do is predict via the softmax which words in this case are towards prior to the nth word and two words after. So, what we want our model to do is to understand what the nth word is and then to be able to predict, in this case, what the two words prior to and after the nth word is. If we can build a model, again, that does this well, then implicitly, we have learned good vector representations for every word which is our goal. Again, to give some clarity on this. So, in this case, the word game is the input sent through the multilayered perceptron. At the top of which is this softmax which tells us the probability of each of the V words, and we would like the model to understand or to predict that if the word game is present, then words: there, was, much, hanging, outside, the, that those are all words that are consistent with the word game. By doing this, we effectively learn via the word vectors. We learn a semantic representation for every word in terms of the word vectors. So, now, to provide a little bit more details on this, so this model which was the first one I talked about is called a Continuous Bag of Words model. So, Continuous Bag of Words (CBOW), and the reason it's called a Bag of Words model is because it actually really doesn't matter what the order of the words: there, was, much, hanging, outside, the, because if you think about what we're doing is we're taking those words and we're simply averaging them and then sending them into the multilayer perceptrons. So, therefore, the order is irrelevant. So, when the order doesn't matter, it's kind of like taking the words and throwing in them in a bag. It doesn't really matter what the order is. So, this is in the nomenclature is called a Bag of Words model. The other model, this is called a Skip-Gram. Here, we're looking at the word game and we're trying to pick the words around it. So, this is called a Skip-Gram. The two models that we're talking about which are two of the simplest are called the CBOW or the Bag of Words model and the Skip-Gram model. In both cases, what we're trying to learn are the set of vectors for each of the words which are represented by the matrix C. So, the matrix C is the set of all vectors for every word in our vocabulary. We use the notation C because we think of this as a code. This is a codebook for all of the words in our vocabulary. Then, we also would like to learn the parameters W and U of a multilayered perceptron and their corresponding biases b and beta. So, all of the parameters in this model, either the Skip-Gram model or the CBOW model are going to be learned based upon a large corpus of data. The thing to notice about this or to think about is that we can learn this model based upon the text itself. We don't need any human labeling of the meaning of the text. We can just take the text and by via the Skip-Gram or CBOW model, we can directly learn these word vectors.