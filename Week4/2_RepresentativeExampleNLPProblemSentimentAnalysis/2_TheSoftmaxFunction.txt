So, in the previous lesson, we introduce this concept of a neural model of text, and that neural model of text leveraged the multilayer perceptron that we have studied previously. However, within the context of our model of text, we introduce this new function called the softmax, which we had previously not seen before in our discussion of the multilayer perceptron. So, this is a new tool, the softmax, and it's probably worthwhile to take a few moments to talk about it and also to connect it to what we have done previously. You recall that when we introduced the logistic regression and when we originally introduced the multilayer perceptron, we were talking about binary outcomes, two-dimensional outcomes. With the text, we have to generalize that because we do not have just two words, we have a vocabulary of V words. So, we have to ask the question, what is the probability of V possible outcomes? Not just binary outcome. The softmax function allows us to do them. However, I want to highlight that this softmax function is not really that different conceptually from the logistic function that we looked at previously for binary outcomes. In fact, it is a generalization. So, here, again, is our neural model of texts. Just to remind you, we have a word which is represented by the m-dimensional vector C_1 through C_m. So, what we're doing is we're taking a word and we're mapping it to a vector, and that vector is represented by the m components C_1 through C_m. W are the weights that give us the mapping from that input vector to our hidden units h, and U is a matrix of weights that gives us the mapping from the hidden units h to the V minus 1 outputs at the top or at the end of our neural network. Then, those weights, Y_1, Y_2 through Y_V minus 1, are sent through that function at the bottom, which is called the softmax. So, this may look complicated. It doesn't look like what we have seen previously. That's called the softmax. We're going to see in a moment that actually it's not as exotic as it might appear. So, with that softmax, we can then quantify the probability that word one, word two, word three, word V in our vocabulary, the probability that each of those V words would be in the proximity of the input word. The way that we're going to do this learning is by looking at text. So, with actual documents, we will know the word, and we'll know the words around it. So, we'll know what the truth is. What we would like to do is to learn a model for this neural model of text, which is characterized by weight parameters W and U, and also the word vectors, the C vectors, those all together constitute our neural model of text. So, what I want to do is to spend a few moments talking about the equation at the bottom, the softmax. So, recall the softmax. I just put it here again just for simplicity. Let's remind ourselves that the way that we set up this softmax is that we set Y sub v corresponding to the Vth word in our vocabulary. We set that equal to zero. So, the softmax is the equation at the bottom. So, I'm just rewriting this. Let's consider the case in which we had a vocabulary that was composed of only two words. So, imagine a very simple vocabulary where it was only composed of two words. Under that circumstance, that arguably complicated equation at the top simplifies considerably. So, we only have two output words, w1 and w2. The probability of the first word, w1, given the code C is the logistic function or the sigmoid function, sigma y_1. The probability of word two is 1 minus sigmoid y_1, which is 1 over exponential y_1 plus 1. So, consequently, if we look at our neural model of text, in many ways, it's just a generalization of the logistic model. So, this is our neural model of text, and this is the special case of that model when we have two words. So, the thing to notice is that in many ways this softmax operation is a generalization of the logistic function or the sigmoid function to the case in which we have a vocabulary of V words instead of just two words. But in the case of two words, this directly reduces to the sigmoid function or the sigmoid function that we're familiar with. So, therefore, we can think of the softmax, which is introduced new here, in the context of neural modeling of text. When we look at the logistic regression that we are much more familiar with, these are actually very close relatives. They're actually intimately related. So, the softmax operation is simply a generalization of the logistic link function. So, it will take a little bit of time to get comfortable or familiar with this softmax function, but actually, it is not a very fundamentally different concept from the logistic regression concept that we already have. So, now that we have the tools of this neural model of text with the softmax, which is a generalization of the logistic function, we're now prepared to ask the question, how can we use this model to learn the word vectors, and then also simultaneously to learn the parameters W and U associated with the multilayer perceptron model of text. We're going to learn all of those together using a text corpus as we move forward in our lessons.