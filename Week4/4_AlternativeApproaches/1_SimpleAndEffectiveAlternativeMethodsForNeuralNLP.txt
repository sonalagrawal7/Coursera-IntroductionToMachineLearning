So, we've introduced several different methods for analyzing text, so we introduced the skip-gram and the bag-of-words model for learning word embedding vectors. We've also introduced the LSTM model which is particularly effective at synthesizing text, at generating text. What's been recognized over the last several years is that while these complicated models particularly the LSTM are useful for many tasks and in fact are required for example for synthesis, it's been recognize that oftentimes very simple models work well for many tasks. So, the last example I want to give on the natural language processing discussions is to just try to demonstrate that for many natural language processing tasks very simple models much simpler than the LSTM and also the convolutional neural network which we've discussed elsewhere. Much simpler models than those can often yield state-of-the-art results. So, let's consider the problem of sentiment analysis. So, sentiment analysis is characterized by the situation in which we're given text, given a document which will represent the ith example of a document where we represented T_sub_i, so, T_sub_i is a set of words associated with the text from the ith document, and then we're also given a label, for example minus one plus one, where minus one may mean that the sentiment is negative and plus one may mean that the sentiment is a positive. Here we are considering a binary example but we can consider other than binary. So, this is a very common problem, we would like to build a model, a machine that can read the text represented by capital T and then it can predict the sentiment. So, the question is, can we do this task using relatively simple models? Models that are much simpler than the LSTM and the CNN. The answer is, that we can and so I'd like to just spend a few moments to demonstrate that, and this kind of underscores that for many tasks in natural language processing simple models work very well. So let's consider text T which is represented by N words w_1, w_2 through w_n, and what we would like to do is a very simple model, so, C w_1 is the code or the vector for word one, C w_2 is the code or the vector for word w_2, an C w_n is the code or the vector for word w_n. Those are the codes, are the vectors representing the words in our document. Let's do something really simple. Let's just average those words. Just take the average of the word vectors across all N documents in our all, average the word vectors for all N words in our document, so we get a single vector after we do that, and then let's just send this into a very simple logistic regression classifier. So here the inputs to the model R is the average of the vectors which will be represented by C_1, C_2, through C_n, those are the M components of the average, of the output of the average of our vectors, we're going to send that into a logistic regression which is characterized by the parameters w and then we're going to represent the probability that the label is plus one in terms of this sigmoid function, sigmoid of the logistic of Y, the probability of class label minus one as one minus that. This is a very simple model simple logistic regression. So, this is bringing together several concepts that we've talked about elsewhere in these lectures, this is logistic regression classifier characterized by parameters w and we're using the concept of word, vectors are word embeddings for each of the words in our document, we're building a very simple model. Take the average of the word vectors, send it to the logistic regression, predict the sentiment, and so that logistic regression is represented or the sigma function is represented by this sigmoid function that we've seen elsewhere in our discussions. So, this has been found as we'll see in a moment to be very effective in practice. The only parameters that we need to learn here are the word vectors for each of the V words in our vocabulary and the weights of the logistic regression, this is clearly much simpler model than the LSTM. It's much simpler than any of the recurrent neural networks, much simpler than the convolutional neural network, but it's been found to be despite its simplicity to be very effective in practice. So, the question that we would ask is, how could such a simple model work well? How could this work well? Now the question the problem that we're solving here is actually a fairly straightforward one, our objective is simply to analyze the sentiment of a document. So, the model to underscore this simplicity. If we look at what we're doing is we're taking the average of the word vectors for each word in our document. So that means that we're not using the word order at all, because if you change the order of the documents, if you change the order of the words in the document the average doesn't change. So, how can this work? So, what must be happening and we'll see in a moment what is happening, is that the model will assign near zero values to the embeddings or the vectors for all words that are not informative for the task of sentiment analysis. So, remember that we're learning word vectors for every word in our vocabulary. If a word in our vocabulary is not important for sentiment analysis what this model will do is it effectively it will implicitly and in practice set all words that are unimportant to the sentiment analysis task to an embedding vector of near zero, which means that, the contribution to the average of all words that are unimportant for sentiment analysis, their contribution will be zero. So therefore, what this model is doing is it's inferring the key words of importance to the sentiment analysis. Notice, I also note that using the average of the word vectors is not the only thing one can do. Alternatively, what you can do is you could do a component-wise maximization and so therefore what I mean by that is just that, if we look at every component in our word vector across all the words in our document, we simply take the maximum of that value across all words in the document. So a document is characterized by a set of word vectors associated with each of the words in a document. What we're going to do is, for each component of the vector, we're simply going to ask which word in our document can has the largest value for that component and then we'll just use that. So, this is what we call component-wise maximization. After we do that, we again get a vector, that vector goes into our logistic model with parameters W, and again we make a prediction. This is a very simple model. So, these are called simple word-embedding models or SWEM. So, that the average, we can either do an average or a component-wise maximization and we can do both actually. We can do an average and a component-wise maximization and then concatenate them. So, what I said was that, conceptually for this to work, implicitly what must be going on is that the model learns that most words are not important to sentiment analysis and consequently the associated vectors of those words are mapped to a zero embedding. Then only the relatively small number of words which are important for the sentiment classification task, only those with relatively small important words have non-zero embedding. So, what I'm showing here is, these are real results on sentiment analysis of the large corpus. SWEM, remember, is our simple model. We're showing SWEM max, which means that we're doing maximization of the components of the word vectors across the document. What we're showing is the frequency of each value in the word embedding and so what you notice is that in the SWEM model almost all components of the word embeddings are zero. So, notice that we're just showing a histogram of the value of the word embeddings and for the SWEM model, almost all words have word embeddings near zero. So you could see that they're almost all near zero and there's only a relatively small number of words that have non-zero value, and those are the words that contribute to the sentiment analysis. GloVe is a different method for learning word embeddings which is different than what we're showing here. It is not based upon the SWEM construct, and here you notice that the GloVe results has a much more broad distribution of values of the word embedding. So, one thing to take away from this, is that the values of the word embeddings or the values of the vectors associated with the words in our vocabulary, those vectors depend upon the method you use to do the learning and different methods will learn different vectors. So when you'd use the SWEM based idea, this simple idea which tends to map all of the or most of the words or the unimportant words to a zero embedding, you get a highly concentrated set of embeddings near zero where another approach using a different methodology here, GloVe, yields a very different set of word vectors. So, this is a very detailed plot not to this slide is just to give a sense of things. We have previously talked about a CNN or Convolutional Neural Network based model for text analysis. We've talked about the LSTM model for text. So,what I just want to give you, and then I've talked about the SWEM, which is the very simple model. So, I just want to give you a sense of the complexity of these different models to underscore that there are many different ways of doing natural language processing based upon these neural models and depending upon what you choose to use, there's very different levels of complexity. So, you notice that the CNN has about 540,000 parameters, the long short term memory has 1.8 million parameters and then this very simple SWEM model has only 61,000 parameters. When we look at computational speed, we also notice that that complexity of the CNN and LSTM also translates into computational time. So we can just notice the speed is not important what exact task we're doing here, but what the key thing to notice is the relative number of parameters and the relative computational speed. Then on the bottom table, this is, again, not something to spend too much time looking at, but what we're showing is results of the CNN and the LSTM based models. So these are very complicated models on the accuracy of predicting the sentiment and then other metrics across the horizontal, the SNL1, the MultiNL1, WikiQA, Quora, MSRP. Those are different datasets and different natural language processing tasks. I give the reference in the figure so you can go take a look at this if you want. The key thing to notice here is that the SWEM based model, which is a very simple model, oftentimes performs as well, oftentimes better than these very complicated CNN and LSTM models. So, the key thing to take away is that this concept of learning embeddings or vectors for each of the words is a very powerful one, can be used across many different types of models, CNN, LSTM, SWEM and others that we've looked at. Oftentimes, we find fortunately that very simple models like SWEM can do very well on complicated but interesting tasks such as sentiment analysis. So as we consider various problems in natural language processing, we need to pay careful attention to examine the appropriate level of model complexity and to make sure that that model complexity is matched to the problem. Oftentimes, very simple models can perform very well.