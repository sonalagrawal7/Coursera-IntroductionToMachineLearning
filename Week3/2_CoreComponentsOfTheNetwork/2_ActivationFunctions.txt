Okay. So, the second core components that we're going to talk about are now activation functions. So, recall the example of a filter being applied to an image input. You get element-wise multiplication and then a sum. That's the convolution operation, the result of that sum is deposited in a pixel in the resulting feature map. But really what exists at every position in that feature map is a artificial neuron, that's taking that input and potentially transforming it into a different value that's passed on to subsequent layers in the network. So, if we take one of those red circles and blow it up, so that's a single artificial neuron, it's receiving some inputs, and in this case, I'm just showing three input values X1, X2, X3. For each of those inputs, you have a respective weight, so those are weights in the convolution, so W1, W2, W3. The result of convolving with those weights that input is a sum 'a', which again is just the element-wise multiplication, and then a sum over all of those multiplications. But instead of just taking that 'a' and passing it on to later layers in the network, there's typically a function that's applied to that input to result in a different type of output. What do I mean by that? Here is just a linear activation, and this is what I've been showing you the entire time when I've been showing you these convolution operations, is that the result of taking that sum, the convolution in getting sum value 'a', then the result of a linear activation function applied to that value 'a' would just be the value 'a' again. So, there would be no transformation of the input to output, and that's a very simple linear activation function. But in reality, what we use when designing these neural networks are non-linear activation functions, and so you may have seen these before when going over logistic regression, or going over the deep multilayer perceptron, these non-linear activation functions. So, when there's an input 'a' to the artificial neuron, rather than just spitting out the same value, you transform that value 'a' in a non-linear way. This increases the functional capacity of the neural network, because it allows it to represent non-linear relationships between features in the input, which can be quite powerful if you're trying to digest those features in a way that can be used to classify or segment in these other real world applications. So, here I'm showing you now on the left, a sigmoid function activation which is commonly used. So, for a given input value 'a', you squash the results between zero and one, so you can think of this as forming a problem. In the logistic regression case, you can think of that as forming a probabilistic output between zero and one. On the right, you have a commonly-used function called the hyperbolic tangent function, which takes in some value 'a' and does a very similar squashing as the sigmoid function, except it collapses the values between minus one and one. So, it squeezes all the values between minus one and one instead of zero and one, but you can sort of think of it as almost the same thing. So, all these have been very powerful non-linear activation functions for neural networks. They have resulted in some problems, so if you notice that at the either top or bottom of these plots, you see these very flat regions. In those regions, for very high variance in the value of the input 'a', you get very little change in the resulting output such that there's very little signal for the neural network as values are changing in dramatically different ways, and this results in a sort of stagnation, occasionally when training these networks that's not particularly useful for learning the representations needed to do classification. So, people have begun to use a different type of non-linear activation, in this case, a rectified linear unit which happens to look a lot like the way a real neuron in a brain might respond to an input. So, it's a very simple function where for any negative value of 'a', the result of the function applied to that input is zero. So, it passes on zero if the total sum of the inputs is less than zero, or returns 'a' if the value of the input is greater than zero. So, the non-linearity is very simple, it's just a maximum value between the value of the input 'a' and zero. But this non-linearity, when introduced to all of the neurons in each layer of the network, increases dramatically the capacity of that network to represent information within the input.