So, at the beginning I explained that deep learning is now a very important area of machine learning. We're now going to get a sense of what we mean by deep learning. So now, what we're going to do here is the same thing we did before but we're going to do it twice. So, starting at the bottom, X is our feature vector. It has M components. So X_i, which is the ith data sample has M features which are represented X_i1, X_i2, through X_iM. Those are the circles at the bottom. Those are the features of our data. Each of the features of the data is multiplied by a parameter vector B, and summed together. We do this K times for K different templates, B_1 through B_K. Then after sending that through a logistic function, we get the probability of each of those K latent features, and that probability is a number between zero one is binary. Then what we're going to do is instead of going directly into the logistic regression to make a binary classification, what we're going to do now is predict what we call layer two latent processes. So now, you can start to get a concept of what we mean by deep because now we have two layers of latent processes. So, at the bottom, we have our data, those data are then mapped to probabilities on latent processes. Then we repeat this process, and then we get a layer two latent processes again represented by probabilities. Then at the top of this model, we then do logistic regression, which is a way for us to do binary classification. Then at the top, we have a probability that the label is Y equal one or Y equals zero. So now, when we look at this, the model is certainly more complicated and we'll talk about that complication and what it implies later. But we at least now can get a sense of what we mean by deep. So, deep learning is a form of machine learning where our model is deep in the sense that it has multiple layers of latent processes. Then in a subsequent example, will explain given in some intuition as to what these latent processes mean. Always at the top, we have a logistic regression classifier, which is giving us the probability of the binary output Y equal one or Y equals zero. But then before we get to that, the intermediate layers correspond here too K latent features at layer one and J latent features at layer two. Now as you might imagine you can go to even deeper models. Here we have a two-layer model. We can go to a three-layer model, a four-layer model, where we have three or four layers of latent processes. So, the key thing to notice is that this model, which is a very classic model, which is called a multilayer perceptron it's also called a neural network, is basically built up by a sequence of repeated application of the logistic regression. So, we now start to understand why the logistic regression is such a fundamental model. Because every component of the multilayer perceptron, which is a fundamental form of a neural network, can be viewed as a repeated application of logistic regression. Now, one of the key things that this multilayer perceptron does, and so the question that you might ask yourself is why would we go to such a complicated model? This is certainly more complicated than the logistic regression. But recall that the logistic regression had limitations. One of those limitations was that the decision boundary between the two the types of data, the Y equal one and Y equals zero was required to be linear, a line. So, with this more complicated multilayer perceptron, it turns out that we can learn decision boundaries which are far more sophisticated. So, here is another example, where again the red dots correspond to data of one label and the blue dots correspond to data of another label. You notice in this case, in this two-dimensional feature space, so in this case, the vectors axis have to components, presented X_1, X_2, and you notice that the blue and the red dots cannot be separated by a line. So therefore, logistic regression would be ill-suited for data of this form. It turns out that often time, data is of this form. So therefore, the logistic regression in its simple form is often inappropriate. So, this motivates going to a more sophisticated model, this multilayer perceptron. In this figure, what we're showing is the probability of the data and so red means high probability of the red data and blue represents high probability of the blue data. This is the output of a multilayer perceptron.