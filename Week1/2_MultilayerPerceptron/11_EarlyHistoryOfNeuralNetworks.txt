Consequently, I think it's a good idea to take a look at the history of the multilayer perceptron. This is meant somewhat as tongue-in-cheek. Other words, not meant to be too precise on things like dates. But it is accurate in the sense that it tells us the history of multilayer perceptrons in neural networks. In that history, I think is a cautionary tale, which will will come to towards the end of this lesson. So, the multilayer perceptron, which we've introduced here, which is at the heart in many ways of deep learning, it consequently seems like very new. It looks interesting. So, one might think that it is something that was just invented. But actually, the multilayer perceptron was invented in around 1960 or 1959. So, it actually has been around a long time. So, this baby is to connote the idea of the birth of the multilayer perceptrons. So, let's look at the history. So, 1960 was a long time ago. Computing power was not what it is today. So, while the multilayer perceptron was developed, its use was limited. The other aspect of that is the amount of data that one had in 1960 was significantly less than we have today. So, from that early birth in 1960, the next big innovation occurred in 1986, which is a technique called back propagation. Back propagation is a method that allows us to learn the parameters of the model in a very efficient way. So, from the back propagation is a very important model from the standpoint of computational, making the model computationally relevant. The other thing to note is in 1986, we had far more significant computational power. So again, what I want to reflect here are what I'm calling the seasons of neural networks. Will become clear why we're calling it the seasons of neural networks as we move move through this. So, in 1986, the back propagation model was developed for the multilayer perceptron. We're having increasing computational power, increasing access to data, this seems like a very bright future. In 1989, a model called the convolutional neural network was developed. We'll talk about the convolutional neural network elsewhere in this module. As we'll be seeing in later part of the module, the convolutional neural network was a key technology for analyzing images. Again 1989, so that was shortly after the back propagation, this was a significant period of optimism. So, as we look at the seasons of neural networks, summer, very hot, very promising period of time. So, around that time, because of the enthusiasm that neural nets had engendered, the people started to apply them in what I'll call the wild, which means applying them to datasets in the real world, and that occurred in the early 1990's. So, now, we've gone from summer into a rather cold winter. The reality is at that time, the models did not perform well. The enthusiasm that existed previously was proven to be unwarranted from the standpoint of how they actually performed in the Wild. There were many reasons for this, but a lot of it was that, I recall that the neural network is much more complicated model than the logistic regression, and the amount of data that one might need to train such a model well is significantly increased. In the early 90's, people really didn't fully understand that these models did not perform well. Around 1995, another key development was something called long short-term memory. It's a bit of a weird name. We will talk about that as well later in this module. Long short-term memory is a very key technology for analyzing data that varies as a function of time. So for example, music or audio signals vary as a function of time. Video varies as a function of time. Long short-term memory is very well suited for that. The key thing I want to reflect in this slide is again that in the mid 90's, there was again some enthusiasm excitement about the potential of neural networks. The other thing to note that I want to highlight here, is that the convolutional neural network, which is at the heart of the modern revolution in neural networks and deep learning, was developed in 1989, a long time ago. The long short-term memory which is now used widely for natural language processing or for analyzing text as we'll see later in the module, was developed in 1995. So, the key technologies which are driving the current revolution in deep learning actually existed at or before 1995. So, this tells us that there were some things that were missing at that time that led to problems as we'll see, and that the underlying technology really hasn't changed. What's changed are some other factors which we'll discuss in a moment. So, after the introduction of long-term, short-term memory, and we had back propagation and we had the convolutional neural network, again, people tried to use neural networks in the wild. You can see this picture denotes fall. Not exactly winter, but it is getting chilly. During this time, again, neural networks did not perform as well as has often advertised. Other methods in machine learning came to the forefront. From roughly around 2005 to 2010, in machine learning, there was, and again this is tongue-and-cheek but fairly accurate, a period of banishment. In other words, in the main machine learning forums, people did not do neural networks or at least did not speak about neural networks at all. The reason why is that the technology really had not performed well enough up to that point, and other methods that were simpler, perhaps easier to understand or to interpret, worked much better. So literally, up until the end of around 2010 or there abouts. In machine learning community, one did not even use the term neural networks for fear that they, if somebody were to use that word, they might be banished or even laughed at. Then because of the fact that neural networks, the term neural networks had come to mean such a negative thing based upon its history and based upon how it performed in the wild with real data. There was a need actually to change the name because if you use, as I said, if you use the word neural network around 2010, people might even laugh at you. So, some people decided to rebrand the technology with the idea that there was now a new moment where this technology really could work effectively but nobody would pay attention to it just because of the name. So, this brought to the fore a new name roughly around 2010 Deep Learning. So, deep learning recall the multilayer perceptron it has a multi-layer architecture remember that that was invented in 1960. So, there's actually not very much new about Deep Learning except the name, it's kind of a cool name, and it's not neural networks, and people were willing to give it a second look. So, we've gone from the banishment of winter, the depths of winter to a spring. Maybe this is the time. Around 2013, a key moment occurred and this, and notice we've now moved to a time of spring in hope. So, the CNN which was developed in 1989 by Yann LaCun and his colleagues. Which dates back to 1989 was applied in the context of two things that did not exist in 1989 and did not exist up until very recently. So, the GPU is a graphical processor unit. This is a computing platform that was originally developed for the gaming community. It turns out to be a very effective tool for parallel or a form of parallel computation. So, the GPU provided a computational platform that we did not have previously. The other piece of this is ImageNet. ImageNet was a dataset or is a data set of images over a million images. So, what we see happen in 2013 is the CNN, the deep convolutional neural network which we will talk about in more detail later in the module, which dates back to 1989 which went through many seasons of despair, was combined with modern computing, the graphical processor unit and was combined with massive quantities of images over a million labeled images. So, these are images for which we had the labels and recall that to do learning we often need labels. In 2013, it was demonstrated that using this technology one could demonstrate a market improvement in performance relative to other approaches, and so this was a fundamental advance. But the thing to notice is that the underlying deep learning technology which really was the convolutional neural network, hadn't changed. Then around 2015 AlphaGo occurred. AlphaGo also was based upon the convolutional neural network and something called reinforcement learning. AlphaGo demonstrated the capacity to play the game Go at a level that exceeded the performance of humans. Now in addition, in the context of CNN in image analysis, we've also exceeded the performance of humans. So, the story that this history tells us is that the underlying technology of Deep Learning which is really a rebranding of neural networks, dates back to 1960 with the multilayer perceptron, 1989 with a convolutional neural net and long-term short-term bond short-term memory, roughly 1995. That technology is decades old. What's changed to constitute this really exciting moment for Machine Learning is that we have brought extraordinary computational power in the form of the GPU and massive quantities of data ImageNet and others. When you bring these together, the performance of these Deep Neural Networks or these deep learning models can oftentimes exceed the performance of humans. So, the thing that I just want to conclude this little history is to remember that we should be humble because at every moment in that history, where we saw spring or summer, only to be followed by winter, there were moments of tremendous excitement that and then disappointment. So, we have learned a lot over those decades and we know a lot more. We now know the importance of computational power. We know the importance of massive quantities of data, but the lesson that I think we need to take from this is that if we do not understand this history, we may be doomed to repeat it. So, there are situations for which this simple logistic regression model on the left may actually be more appropriate than the Deep Neural Network. So, one of the keys as we move forward with this technology is to understand where the Deep Learning is effective. What type of Deep Learning is effective and also where simpler models might be appropriate. So, there's a very famous idea called Ockham's razor and what it basically says is that if you have two ways of describing data in other words, if you have two models of data and they are equally good at describing the data, you should always choose the simpler model. One should always look for simplicity, and so when we look at the simple but often effective logistic regression and the more complicated Deep Neural Network, one must always recognize that it's probably a good idea to look at data from both perspectives and where the simple model works, the simple model should be used. But there are often situations as history has shown us for which these deep architectures do indeed provide performance which is well beyond that of simpler methods. So, as we move forward in this module, we'll start to study in greater detail why it is that these more sophisticated deep architectures yield performance that has generated so much recent excitement in Deep Learning and Deep Neural Networks.