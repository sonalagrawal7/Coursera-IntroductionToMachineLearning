Multilayer perceptron, which we're going to introduce now, is actually a rather direct or natural extension from logistic regression. So remember that x sub i, corresponds to the ith data example, and it has m components, x sub i-1 through x sub i- m. Recall that in logistic regression, what we did was we took each component of the data, multiplied it by the parameter, added that up and mapped it to a variable z, and then that variable z was sent through a sigmoid function which is represented by the symbol sigma. What we're going to do here is the same concept as in logistic regression, but instead of doing it one time, we're going to do it k times, and so now, what we're doing is we're doing effectively K implementations of what we did with logistic regression, which are going to map the data x to a k dimensional vector, which is represented by the probability from zero to one of what we call k latent processes or features associated with the data. These are called latent because these are representative of things that in the data characteristics of the data, we cannot in principle observe. Then those k latent features are sent through a logistic regression model to ultimately yield a binary probability for the classification of the data, and so this is very much like logistic regression but what we have done is introduced instead of doing logistic regression directly from the data to the binary outcome, what we're doing is we're doing logistic regression, we're first going from the data to k latent processes or the probabilities on those k latent processes and then that corresponds to a k dimensional feature vector and then that k dimensional feature vector is used to apply logistic regression, which is then ultimately yields a outcome, the probability that the data corresponds to class y equal to one. So, what this can be viewed as, as logistic regression being performed on the k features, instead of being performed directly on the raw in the initial data. Now, just to give some motivation for why doing the simple logistic regression may not be as effective and to motivate why the newer model, this model that we've introduced here may be more effective. So let's go back to the problem of digit recognition and let's examine the situation for which our goal is to perhaps identify handwritten examples of the number four. So what you're seeing here are four examples of handwritten representations of the number four, and so you can see that people have many different ways of writing the number four. If we were to use logistic regression, which as you recall, takes a single filter B, and does an inner product with the data xi, what would typically happen is you might find that that filter here represented by the values of the pixels in the image to the right, might look like an average four. So this average four doesn't look like any of the fours. So the idea of building a classifier based upon a single filter of this type seems undesirable. So alternatively, we might consider a situation in which we have three intermediate filters, so if we go back to our multilayer perceptron, we can take our data, which here corresponds to the pixels of the image, and instead of multiplying them by filters that correspond to one average four, we might consider k equal to three filters, which correspond to four different ways in which one might draw the number four. So, what we see on the right are examples of what three filters might look like, and then those, the latent features that are inferred will then be used to ultimately make a final prediction about whether the number four is present, so the key idea in going to this new model is that we're no longer going to limit ourselves to a single filter. B to examine the data, we're going to introduce k filters.