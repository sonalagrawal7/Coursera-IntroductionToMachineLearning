So, now what we would like to do is take a look at the multilayer perceptron, and add some of the math that drives that model. The aspects of the mathematics are worth going into because they are at the heart of most neural models that we'll be interested in. So, while we're talking specifically about the multilayer perceptron, these ideas extend to other types of models. So, the key thing that we would like to try to communicate is the idea that the multilayer perceptron is a natural extension of logistic regression. So, you recall that in logistic regression, we had data which was represented here by x sub i, x sub i one through x sub i, m, which corresponds to the m components of the ith data samples. So, x sub i is the ith data sample. In the logistic regression, what we did was we mapped that input data xi to a variable zi, through an inner product, or a dot product between the data xi and a vector b, and then, so there was a single if you will filter, that was applied to to the data. The idea in the multilayer perceptron is that instead of just projecting the data xi with one template vector as we did in logistic regression, what we're going to do is consider k filters or reference factors b1 through bk, and we're going to take xi, and we're going to take the inner product of xi with b1, xi with b2, and all the way to xi with bk, inner product with bk. So, the idea is that instead of taking the data, and taking an inner product with the single vector as we did in logistic regression, we're going to do it with k, vectors b1 through bk, where going to then add biases in the same way that we did with logistic regression, and then we will have outputs of each of those operations which will be zi1 through zik. So, instead of realizing as we did with logistic regression just a single output, here we have k outputs which are k features from layer one. Those features are then sent through a logistic function which is shown on the top right, which is a mapping of a real number, a number that can be anywhere from negative infinity to positive infinity, a real number, we squash it or map it to a number between zero and one, and the reason we do that is is that now we're going to be introducing probability on k latent features. So, the way to think about this is that we have a data xi, and then there are k latent features or processes that are responsible, or at least from a modeling perspective are represented as being responsible for the data. After this, the inner product, and then going to the logistic function we then have to the sigma, that's called a sigmoid function, we then have a mapping for each of those k-features to a probability from 0-1. So, that basically represents the degree, or the degree to which each of those latent features a is represented in the data. Then, those k-latent features or, those k-latent probabilities are then sent through a logistic regression type model in the same way that we did previously with logistic regression, and now we have a single template filter C with which we take the inner product of those latent features, we get a variable Zeta sub i, that that Greek symbol is called Zeta. Zeta sub i is then again sent through a sigmoid function which tells us the probability of the data being associated with a particular binary label. So, the key thing to notice about what we're doing with the multilayer perceptron is that instead of taking the data xi, which has m components xi1 through xim, and then directly using logistic regression to map that to a probability of a binary outcome. We have an intermediate step where we introduce k-latent features, and then those k-latent features are then subsequently sent through a logistic regression type model to ultimately give us a probability of a binary outcome. This added model sophistication through that intermediate or middle layer adds significant modeling flexibility. In particular, it allows us to consider non-linear decision boundaries in feature space. Therefore, oftentimes, has been shown to be more effective from the standpoint of yielding more effective results.