Another question that I try to give a sense again is, why is deep learning so powerful? What can deep learning do that is not done easily by other model? So, if we go back to this model, this is a model that allows us to make prediction about whether a person of interest will like or dislike certain documents, and the like or dislike of those documents in this example are controlled by whether that document is characterized by certain topic and meta topic and whether the person of interests likes those particular topic and meta topic. Now, we can now think of how we could use this multi-layer perceptron to analyze multiple people. So oftentimes, we're interested not in predicting how just one person is going to like documents, we're interested in how many people might like documents. So, a way that we can think about this, is the first and second layers of our model recall are looking for the presence of topics and meta-topics in our document. So, therefore the first two layers of our model are actually characterizing the documents. They don't really characterize the person they characterize the documents themselves. So, consequently what we could do and they analyze if we wanted to build models for many different people, what we can do is the first two layers of this model or the parameters associated with the first two layers of this model can be shared or reuse in models of multiple people. So, therefore from the standpoint of model building, so remember when we do model learning we have to use training data to learn. So, if I can reuse the first two layers of this model across many different people, in other words, across data from many different people, then that means that I have the opportunity to leverage much more data than I otherwise could to learn the parameters of the first two layers of our model. Then, the top layer, which characterizes whether a given particular person likes particular meta-topics, this has to be person dependent. So, the thing that I want to try to communicate here, is that as we look at this deep architecture, the lower layers or the layers in the model that are closest to the data at the bottom are characteristic of the data. The layer at the top is characteristic of the person. So, we call this transfer learning. So, if I have data from many different people, I can transfer data from one person to the other when learning the parameters of the bottom of the model. Then I only use data from the person of interests to learn the parameters at the top. So, this transfer learning allows a much more efficient use of data. So, in addition to the other advantages of deep learning that we talked about earlier such as the ability to learn a more general decision boundary, a more sophisticated decision boundary between the features of data characteristic of label y equal one and y equals zero. The other significant advantage of deep learning and here the multilayer perception, is it offers us the ability to do transfer learning, which allows us to use our data in a much more efficient way instead of using data from one person to build one model for one person and then use data from another person to build a model for that person, what we can do is we can take the data across all of our people and therefore have much more data to share that data or transfer that data across the different subjects and then use all of it to learn the bottom parameters of our model and then only use the data associated with a given person to learn the relatively small set of parameters at the top. So, now thinking back to the logistic regression, which was our starting point and our building block algorithm, this is a very simple model and it is therefore a nice place to start. However, it doesn't have the capacity like this multi-layer perceptron to do transfer learning. So, with the logistic regression, we have separate parameters B1 through BM. If we want to build a predictive model of whether a person likes a document or not. We really do not have the capacity to do transfer learning in the same way.