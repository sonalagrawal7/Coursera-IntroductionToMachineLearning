Oftentimes, in machine learning, we have a question of model selection. There may be multiple different models that one might use to represent the data. You have to make choices. The way to make those choices oftentimes, is gained through experience. So, this is with the tools that we have developed thus far. We can look at a particular example of model selection that we might consider with the tools that we have. So, recall that x sub i is the ith data sample. It has M components, so we represent the data as xi1 through xiM, the M components of the data. In the logistic regression, we take the data, we take an inner product with the weights b1 through bM. So, we basically multiply each component of the data xi, with a respective component of the vector b. We sum those products up, we then get a zi, we typically then add a bias. Then finally, this is sent through a sigmoid function denoted by Sigma from which we get a probability of an outcome. That's a very simple model, it may be very effective in some situations. Another model that we now have at our disposal, is the multi-layer perceptron, which we see at the right. So again, at the bottom we have the data xi1 through xiM, the M components of the data. Instead of directly going through with logistic regression to make a classification decision, in this case, we show two intermediate layers of latent features. Then at the top, the latent features at the top are again sent through a logistic regression to make a classification. So, here through this picture, we can see the connection between the logistic regression and the multilayer perceptron. The multilayer perceptron has logistic regression in it at the top of the model, but there are intermediate features that are manifested through this neural network or multilayer perceptrons. So, the question that one often what we'll have is, which model to use? So, this is something that in machine learning is called the Bias Variance Trade-Off. The way to think about this is that, if you look at the logistic regression model, it's a relatively simple model. The number of parameters in the model is relatively small. The multilayer perceptron, by contrast, is a rather complicated model, or rather sophisticated model. But in consequently, it also has a lot more parameters. So, the variance, what we think about when we talk about variances is that, if I were to take different examples of data. If I were to look at a situation where I have N different data samples, and I were to train my model using those data samples. If I were to look at different instantiations of those N training examples, how much variation would there be in the model? So, with the simple model, because of the fact that it is biased towards simplicity, the amount of variance that we would expect to see in different training scenarios, with different training of N samples of data. We would expect in the logistic regression case that the amount of variance that we would see that between different training scenarios would be small. On the multilayer perceptron case, given the added complexity, we might anticipate that it has greater variation from training on one example of data, to training on another example of data. So, this is called the Bias Variance Trade-Off. We oftentimes, will bias a model towards simplicity such that there is not a lot of variants. However, that bias or that simplicity, might undermine performance. So, to show a simple example of this MNIST, is a very widely used dataset for digit recognition. So here, we're looking at 10 digits, zero through nine, and the objective is to take one of those little pictures of a digit, to send it and the pixel that correspond to that little picture, are going to be the elements of our data xi. So, a simple model that we might consider to do is the logistic regression. In that case, the pixels of the image are directly weighted by b1 through bm, and then sent through a sigmoid, and then a classification decision is made. If you do this. If you try to solve the MNIST problem with the logistic regression, you would, for example, see an accuracy of around 91%. If we use by contrast the multilayer perceptron, we can see that the performance oftentimes can improve to 96% accuracy. So, this is an example where the added sophistication of the more complicated multilayer perceptron. Here actually, there's only one additional layer relative to the logistic regression, yields a significant improvement in performance. The reason that this is true, we've talked about this elsewhere, is that the logistic regression is restricted to a linear classifier where the multilayer perceptron allows a nonlinear classifier. That nonlinearity in the classification decision yields improve performance. The other thing that I should briefly note, is that this is a 10 class problem. So, we have digits from zero to nine. So therefore, this is not a binary problem, this is a 10 class problem. For simplicity, I'm using the same binary logistic regression and multi-layered perceptron pictures. But, in the actual implementation at the top of the network, we have a 10 class classification problem or algorithm, which is manifested as a generalization of the logistic or sigmoid link function and it's called the softmax. We'll talk more about the softmax, elsewhere in our class.