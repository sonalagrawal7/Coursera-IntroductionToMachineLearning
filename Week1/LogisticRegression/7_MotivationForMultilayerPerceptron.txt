What we would like to do now is to move beyond logistic regression, which is a simple machine learning model, and move to more sophisticated model, in particular, something called a multilayer perceptron. This is one of the most basic forms of deep learning, it's one of the most basic forms of neural networks. So, in logistic regression, recall that, what we do is we have data Xi, we take an inner product of the data Xi with a set of parameters b, which we call a filter, and that inner product Xi with b is then added to what we call a bias or just a number B nought, which is an additional parameter that we need to learn. That ultimately live gives us Zi, which is then sent through this logistic function or the sigmoid function which converts Zi to a probability. In particular, the probability that the label is equal to one. Now, this is a very powerful model, the logistic regression. But, we would like to move beyond it. So, the question is, what's wrong with logistic regression? So, it turns out that logistic regression is a very good model in situation in which the delineation between class one and class zero in our binary problem can be separated by a line. So here, a line or a plane. A line in two dimensions, a plane in three dimensions, and what's called a hyperplane and higher dimensional problems. Here, we're looking at a situation in which the data are two-dimensions. So, we're looking at X sub one and X sub two, which are the two dimensions of our data. Every point here corresponds to a data point. So, every point corresponds to an example of X1, X2, a particular data. So, recall that that logistic regression, is solving a binary problem. So, here red corresponds to one class and blue corresponds to another class. In this case, a line does an effective job in separating or distinguishing between the red and the blue classes. So, for the data of this sort, logistic regression is appropriate, because it designs a linear classifier. However, there are situations in which the data are not well separated by a linear classifier. So, here is an example where again, we have the blue dots corresponding to one class, and red dots corresponding to another class. You can see that the decision boundary between the red and the blue dots is much more complicated than a line. In other words, simple line will not separate these. So, what we would like to do now, is to build a model that is capable of building decision boundaries between the class one and class zero that is more sophisticated than what a linear classifier can do. This is our motivation to go into more sophisticated models and in particular, the multilayer perceptron. The key thing to take away from this is that the logistic regression, which is a very nice model, it has a limitation that it only works effectively whenever a linear classifier is effective at distinguishing class one from class zero. Any other case such as the one you're looking at here in which the decision boundary is more complicated for which a linear classifier will not be effective. We want to use more sophisticated model, this motivates the multilayer perceptron, which is a natural extension of the logistic regression.