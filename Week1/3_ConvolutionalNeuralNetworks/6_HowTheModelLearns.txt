So, if we look at this model, what we now have is a model characterized by parameters. The parameters of the model are the filters at the bottom, the filters at the second layer, and the filters at the third layer, Phi, Psi, and Omega, and then finally, the parameters of our classifier W. So, on the right, the last bullet is to emphasize that the parameters of this model, which are unknown and which we wish to learn, are Phi, Psi, Omega, and W. So, now, the assumption, the way we do learning, is we assume that we have access to a large set of data, a large set of labeled data. Now, recall that I said that one of the key things that has made this technology, which was developed in 1989, one of the key things that has made it so powerful in recent years, is access to massive quantities of data. So, here, that data is represented by the pair I_n is the nth image, and y_n is the label of the image. Recall that from the very beginning of this module, we always talked about the concept that we needed labeled data to do learning. Here, we assume we have capital N examples of images and labels. We look at the second bullet, where for simplicity, much as we did when we studied the logistic regression in the multilayer perceptron, we're going to assume that the labels are binary, either plus one or minus one. We're doing that here purely for simplicity. In other situations, the number of labels could be very large, hundreds, thousands. But for simplicity, let's just leave the labels to be plus one or minus one, binary, to be consistent with our discussion of logistic regression in our prior discussion of the multilayer perceptron. So, now, looking at the third bullet on the right, what we're going do is we're going to constitute, what we call, an energy function, what I represent by E, which is going to compute, what we call, the loss between the true label, y_n, and our model label, l_n. So, if we look at the figure on the left, we see that at the top of the network, we take the feature maps at the top of the network and using our model or our classifier with parameters W, we make a prediction of the label script l. So, what we could do is we can calculate a loss. The loss is a measure of the difference between the true label, y_n, and our predicted label, l_n. What we would like to do is to make that loss as small as possible, and there are many ways that one can choose that loss. The loss is just simply a measure of the difference between y_n and l_n. Again, there are many different ways that one can compute that. So, our goal here, looking at the simple equation after the third bullet, is to try to learn, or identify, or compute, the parameters, Phi, Psi, Omega, and W, that minimize that sum over loss. The loss, again, is essentially the difference between our accuracy of our predicted labels and the true labels. This is what we mean by learning. So, when we talk about machine learning, this figure, this slide, really brings it together. The left figure, our convolutional neural network, is the machine. That's what we're trying to build to try to characterize images. The learning part is manifested by taking data here and examples of image I_n and the label y_n, and trying to learn the parameters. So, what we mean by learning is to learn the parameters of our machine, of our model, such that the difference between the prediction of the model, l_n, and the true label, y_n, is small. So, in the last bullet, we say that we put, what's called, a hat sign on the top of those symbols, which is the, anyway, a small little symbol like a hat. Those are our predictions. So, our goal is to try to design, or predict, or estimate, those model parameters, Phi, Psi, Omega, and W, such that the total loss manifested in the middle equation, between our predicted labels and the true labels is small. This is what we mean by learning. Now, why is this hard? It turns out that this is a very challenging problem, which is part of the reason that we've had that long history of machine learning. So, when I talked about the seasons of machine learning, and we went through springs and winters, and more springs and summers and winters, the reason that we went through those periods of enthusiasm and then feelings of desperation was because there were many challenges to doing this learning, to estimating these parameters. One of the key aspects of that is that this search, the estimation of those optimal parameters to, try to estimate Phi, Psi, Omega, and W, the number of parameters that we have to estimate is massive, very, very large. So, consequently, trying to estimate those parameters is very difficult. So, just to give you an example of what that might mean, here is a situation in which we're only looking at two parameters. So, along the two-dimensional bottom axes, we might be looking at parameter 1 and parameter 2. Along the vertical axis, we're looking at the value of that function E, which is a measure of the quality of the fit of the parameters to the data. So, what you notice here is that there are multiple valleys, multiple areas where the parameters might lead to relatively small error, E. So, what we find is that there are many different sets of parameters which give what we call local optimal solutions for the model. So, it is very difficult to move in this parameter space to actually optimize for the parameters. Here in this simple example, I'm only showing it for two parameters. Again, the vertical axis is the quality that the error, E, and the other two axes represent the values of the parameters. So, in two dimensions, in this simple example, I can actually draw a picture and look at it. But for the real problem, the number of parameters in Phi, Psi, Omega, and W, could be millions, millions of parameters. So, consequently, we can't even draw a picture like this, and it is extremely difficult to search in that massively high-dimensional parameter space to find the parameters that yield the best match between the predictions of the model and the truth of the labels. So, it has taken literally decades of research and study to learn how to do that in an effective way. That research and the developments that have been made during in that period have played a key role in the success of modern deep learning, in addition to the computational power and the access to data.