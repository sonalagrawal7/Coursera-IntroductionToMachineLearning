Now, the interesting thing is that everything that we learned in our illustrative example essentially directly carries over. So, let's again start by studying the convolutional neural network which is designed to study and analyze images. So, i sub n is meant to represent the nth image, which is meant to emphasize that we're not going to do this for just one image, we're going to do this for thousands, and millions, tens of millions of images. So, to do learning, we're going to use many, many images, and by studying these in analyzing these images, we'll be able to learn a model. So, within the context of that learning, the model is given access to each of those images, and so just for depiction, we again show one of those toy images, but of course it will be a real image, and i sub n is the nth image. Now, remember that the first part of this model was to convolve atomic elements, remember in the form of shapes in the toy example, convolve those atomic elements to every location in the image, every two dimensional location in the image, and then to constitute a feature map, where the feature map reflects how strongly that atomic element is manifested in the image. So, what we're going to do is the same thing, but now I do not know the atomic elements. So, what I'm going to do is, I'm going to represent them by parameters that we're going to learn. So, phi 1, phi 2, through phi k correspond to k filters. Instead of them being shapes, as they were in our toy example, these are just going to be the parameters of these filters are just the pixel values. So, what we're going to try to do is learn the pixel values associated with filter phi 1, filter phi 2, and filter phi k. Now, I want to learn what those parameters are. I want to learn the values of the pixels for each of those filters. Now, let's assume we knew what they were. At this moment, let's just assume we knew phi 1 through phi k. If we know those pixel values, we can shift them to every location in the image and constitute a feature map exactly in the way we did for our toy example. So, the way that this is depicted in the second line, is that In is the Inth image. This process of filtering which manifests these feature maps is a function of the filters phi 1 through phi k. Then, this yields a feature map, which I'll call m sub n, where m sub n is the feature map, the stack of feature map at layer one for the nth image. Then, we can repeat this process. So, now, the stack of feature maps from layer one is convolved with a layer two set of filters which I'll represent as psi 1, psi 2, through psi k. Again, I don't know what those are. I don't know what the values of those filters are. What I mean by the values is, each one of those filters: psi 1, psi 2, through psi k is a digital mini stack of images, stacked to match the feature map from the layer below. I don't know what the values are, but if I didn't know what the values were, I can convolve each of those filters, psi 1 to psi k, with the layer one feature maps, and then manifest layer two feature maps, which are a function of the feature maps from the layer below, m sub n and the filters psi 1 through psi k from the second layer. If I knew what those were, if I knew what psi 1 to psi k are, I can do that operation. What we're going to do is, we're going to learn those parameters. We're going to learn psi 1 through psi k. Then, this process can be repeated.. We go to the layer three, and we have filters omega 1 through omega k. These are again stacks of filters. They have values. We no longer have those simple shapes, but they have values in the same way. We can convolve those layer three filters, with the layer two feature maps, and then finally, get a feature map at the top layer, which we call g sub n, which is the top layer feature map, which is a function of the feature map at the layer below and the filters at the layer below. Then, finally based upon the features at the top, we can build a classifier which will provide a label for that image. So, script L is meant to represent the label. The label is a function of the features at the top, g sub n and parameters w, which reflect the parameters of our classifier. So, now if we look at this, first of all, the structure of this model is exactly the same, as in our toy example. The only thing that distinguishes this from the toy example is that we're no longer assuming those basic atomic shapes. Instead, we have these parameters. The parameters are the pixel values at every location within those filters. In the process of convolutional filtering is exactly the same, and then at the top of this model, we have the top layer feature maps, which we call g sub n. Those are sent into a classifier which is characterized by parameters w. That classifier could take a form that we're familiar with. For example, that classifier could be a multilayer perceptron or it could be a logistic regression. But in any case, it's characterized by some parameters which we denote by w.