In module 2, we learned about how a neural network
learns from data. By defining a loss
function that quantifies as a single number how
poorly a model does, we can use Stochastic gradient
descent to adjust each of the parameters in our network to improve the
model's performance. Throughout module 2,
logistic regression was used as a running example to
illustrate these concepts. In this section,
we're going to put these concepts to use
and actually code a working model that uses logistic regression to classify the digits of the
MNIST data sets. Once you've trained
it, we can evaluate our models performance
and as an added bonus, visualize what it
actually learned. We'll first write our code with basic low-level
PyTorch operations, but we'll finish by seeing how we can rearrange our code in a more object-oriented
manner using higher level APIs to build
our model more efficiently. As an assignment,
you'll be extending our logistic regression model
to a multilayer perceptron, which was introduced in module 1. You should find that
it performs better than our simple logistic
regression model.