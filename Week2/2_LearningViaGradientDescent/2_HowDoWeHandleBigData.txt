So, we have previously talked about Gradient Descent, and Gradient Descent has a fantastic optimization algorithm. But we're going to run into trouble when we hit truly big data. So, in this video, we're going to talk about how we actually can handle big data from an optimization perspective. So, in order to do this, let's actually go back to our optimization goal. So we have this network, we have our training data and we want to find the best parameters for all data points. So we have this b star as the value that minimizes our function on the right, our average loss function, and we have just talked about how we can do this through gradient descent. Calculating the gradient requires looking at every single data point. This is a big drawback because we need to calculate the gradient or multidimensional slope over all data points, this is going to require looking at all of our data. And you can see this mathematically here if you want by just showing that the gradient over all of our data points is just going to be the sum of the gradients of our individual data points. This just happens because a gradient is something known as a linear operator. This can be very problematic in big data. So, MNIST which we've previously talked about this with our dataset of images of numbers, it has only order 60000 images. When you go to these very large data sets, we can actually have millions of examples or billions depending on the application and this is very feasible. So, this is really problematic if we're going to look at every single data point. If we have to run through a million images just to calculate the gradient to take one little update, this is not very scalable. So, let's just go through and approximate the gradient, and we don't want to look at every single data point to update our parameters. So there's a question, do we actually need to do that? So, just think about a strategy and just run it through and see what would happen. So, we're going to take a single example j. I'm going to pick j at random, and i'm going to use it to approximate the gradient. What we're showing here, is that we're just going to say the gradient over average loss. If we take the gradient over every single data point, we're going to approximate it by taking the gradient with respect to a single data point, and it's a randomly chosen data point. So, the reason we want to do this is that this is incredibly faster. So, if we think about this MNIST dataset, this dataset we're looking at just images of numbers of handwritten numbers, this is 60000 times faster. We can run this 60000 times. And dataset with a million images, this is a million times faster literally a million. So, does this work? Right? Is this actually a good idea, and what would this look like? So, let's actually go through a visualization to show what's actually happening here when we run Gradient Descent versus Stochastic Gradient Descent. So, I've just started the simulation here on gradient descent. So, what's happening in gradient descent? We're making very smooth progress, we're taking little steps that are going down the hill. Now, we can see that it's making good progress it keeps going and we're going to get to the bottom eventually and it's going to make very smooth progress, we know we're going to be improving. If we run Stochastic Gradient Descent, we can run it and we get way faster updates. Sometimes we're actually moving in the wrong direction, sometimes our gradient approximation is not very good. But on average we're moving in the correct direction and we get near the minimum very quickly. What we're showing here, is that we can run updates many times faster in Stochastic Gradient Descent. But in reality, this is on a 1000 data points, this is a simple problem. I've actually drastically slowed down Stochastic Gradient Descent so that you can actually see what's happening. It's actually many times faster than this and as we get more data points it's going to be an even bigger difference. We can see the Gradient Descent is still going and we're not going to let it finish because we just don't want to sit here and watch it. So, why does this work? One of the reason this works, is that data is often redundant. If we go back to this MNIST example, where we had this dataset of handwritten digits, we have a bunch of people that have written zeros, we have a bunch of people have written ones, twos, threes, fours and so on, and we have 60000 images. But in reality we only have 10 different types of images. There are a few different ways you can write a zero, a few different types of ways you can write a four and so on but there's only so many different types of digits. If we want to understand about a digit, do we have to look at every single digit to get an approximation to a gradient or do we just need to have a few in order to get an idea of what's going on in this dataset. We can take advantage of this redundancy. And just to talk about mathematically what's happening here, is on the left we're showing this gradient descent strategy with this is the same mathematical description that we had in a previous video. So, we're going to start with an initial b-0, we're starting with an initial location of our parameters and we're going to calculate the gradient over all the data and then we're just going to iteratively update. So, we're going to take a step in the negative direction of the gradient. I was just going to repeat steps two to three until we're good enough. If we want to change this to Stochastic Gradient Descent, we have something that's very similar but much faster. We're going to start with an initial value and we're going to choose a data entry j at random we're just going to pick one data entry and it's going to be chosen completely at random. We're going to estimate the gradient by the single data point, and we have a hat here over our gradient because it's now an estimate and we're going to iteratively update. And I want to point out that the iterative update in stochastic gradient descent is exactly the same as the iterative update in gradient descent. The only thing that's changed is that we have an estimate of what a gradient is rather than our exact gradient. Now, we're just going to repeat steps two to four until the solution is good enough. So let's step back and think about what's actually happening here. Let's start by noting that if we take the expectation over our gradient estimate which is just a gradient of a single data point where we've chosen this datapoint completely at random that it's exactly equivalent in expectation to our true gradient where we've calculated over every single data point. So if we start Gradient Descent and Stochastic Gradient Descent at the same place, how do their updates vary and how do the expectations of their updates actually vary. So if we think about what's happening in these updates, they're going to go to the same place and expectation. So, if we take our Gradient Descent update and we take our Stochastic Gradient Descent update, they're going to be on average going to the exact same spot. So on average there's no difference between them. But the differences that Gradient Descent will go to the same spot every single time because we're calculating our gradient fully. Whereas Stochastic Gradient Descent we're going to get a lot of variants in where we're moving to. On average we're going in the same place as Gradient Descent but we're actually going to have some variance. So sometimes we're going to move a little too far, sometimes we're going to move a little too small, sometimes we're actually going to move in the wrong direction but on average we can do exactly the same thing. We can go back and think about this while we're doing these visualizations again. So we're looking at Gradient descent very smooth. No variance in our updates we run Gradient Descent from the same starting point every single time we are going to get the exact same result, it's going to be very smooth. Stochastic Gradient Descent once again we're moving on average in the same way Gradient Descent is but we're having a lot more variance. Sometimes we move in the wrong direction, but that's okay because on average we're moving in the right direction and we can do this much faster than we ever could of Gradient Descent. So just to recap, we have shown this figure in the schematic several times now about learning the model parameters. We have our training data and we also have some sort of network or model that we want to learn. We said that we're going to combine our train data with this network in order to learn our parameters and before, we just said that we're going to do this through some sort of optimization, we are going to define our mathematical loss. But all we're going to do in practice is just use stochastic gradient descent to the step where we're going to go from our network to our learn parameters, we're going to use Stochastic gradient descent or an alternative stochastic gradient method and this is what we do in practice on almost every single network. So, just some conclusions from this video today. Just to reiterate, Stochastic Gradient Descent can update many more times than gradient descent. If we have 60000 data points, we can run 60000 updates of Stochastic Gradient Descent and about the same amount of time it takes to run a single update of gradient descent. We're going to get near the solution very quickly and often all we need to do is get near the solution, we don't need the exact solution we just need to be very near it. And this is going to allow us to scale to big data. And the reason this is so helpful to scale to big data is that our update time doesn't increase with our data size. If we double the size of our dataset, it's going to take about the same amount of time to run this Optimization Algorithm. Whereas in a lot of Classical Optimization Algorithms such as Gradient Descent, it will take much longer. In practice, we're often going to use a mini-batch. Which means that instead of using a single data example, we're going to run a few data examples to estimate the gradient and this will reduce variance. But overall, Stochastic Gradient Descent is just going to allow us to scale to big data in a very simple way.